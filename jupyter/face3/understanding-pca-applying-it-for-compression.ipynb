{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a4f351",
   "metadata": {
    "papermill": {
     "duration": 0.029917,
     "end_time": "2022-06-01T18:52:41.952544",
     "exception": false,
     "start_time": "2022-06-01T18:52:41.922627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this notebook we are going to visualize and understand an application of the Principal Component Analysis (PCA) algorithm and how it works in the background through the process of compressing and decompressing image files to reduce memory usage. Since we are also going take a look at the math behind it, it's highly recommended that you have at least a basic understanding of the principles of **matrix operations, eigenvectors, eigenvalues, derivatives and statistics**.\n",
    "\n",
    "    \n",
    "\n",
    "### Optional (recommended)\n",
    "Now, if you do not feel comfortable with or just need a brief recap on those subjects, here are a few videos that may help you. I know this may seem like a lot but personally I highly recommend that you take a break to review the following contents if needed, since these are all valuable knowledge in the field of Computer Science.\n",
    "\n",
    "* Matrix multiplication:\n",
    "https://www.youtube.com/watch?v=OMA2Mwo0aZg\n",
    "\n",
    "* Concept of the derivative:\n",
    "https://www.youtube.com/watch?v=-ktrtzYVk_I\n",
    "\n",
    "* How to take the derivative of a function:\n",
    "https://www.youtube.com/watch?v=QqF3i1pnyzU\n",
    "\n",
    "* The chain rule:\n",
    "https://www.youtube.com/watch?v=H-ybCx8gt-8\n",
    "\n",
    "* Matrix differentiation:\n",
    "https://www.youtube.com/watch?v=e73033jZTCI\n",
    "\n",
    "* Eigenvalues and eigenvectors:\n",
    "https://www.youtube.com/watch?v=glaiP222JWA\n",
    "    \n",
    "* Variance:\n",
    "https://www.youtube.com/watch?v=x0rmUXWtSS8\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f97014",
   "metadata": {
    "papermill": {
     "duration": 0.031827,
     "end_time": "2022-06-01T18:52:42.012958",
     "exception": false,
     "start_time": "2022-06-01T18:52:41.981131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is PCA?\n",
    "\n",
    "<font size=3>The Principal Component Analysis (PCA) algorithm is one of the most widely known algorithms out there and has applications in a vast variety of fields regarding the use of data. It works by trying to find the best way to fit a n-dimensional set of parameters into a lower k-dimensional representation and can be used for **compressing data, speeding up models, reducing memory usage and generating plots**.\n",
    "\n",
    "    \n",
    "<div style=\"width: 100%; text-align: center\">\n",
    "<img src=\"https://www.researchgate.net/profile/Mohammad-Abu-Alsheikh/publication/262452270/figure/fig4/AS:667663848194057@1536194874955/A-simple-2D-visualization-of-the-principal-component-analysis-algorithm-It-is-important.png\" width=400 align='middle'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f28b",
   "metadata": {
    "papermill": {
     "duration": 0.031725,
     "end_time": "2022-06-01T18:52:42.081089",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.049364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Intuition\n",
    "<font size=3>\n",
    "    \n",
    "Intuitively, the PCA algorithm projects each point of data onto the best-fitting line across all data points in the dataset, in which the resultant vector that represents the direction of that line is known as the **principal component**. Each projection is given by the linear transformation (dot product in this case) of a high-dimensional *x<sup>1</sup>* vector by an *u<sup>1</sup>* component; therefore, let *z<sup>1</sup>* be the the low-dimensional transformed vector representation of *x<sup>1</sup>*, we have that <i><b>z<sup>1</sup> = u<sup>1</sup> * x<sup>1</sup></b></i>. Remember, at the end of the day we should choose the component with **highest variance** as the principal component as it can represent the data more accurately.\n",
    "\n",
    "*E.g.* for a 2-dimensional dataset, that is each sample contains 2 features *x<sub>1</sub>* and *x<sub>2</sub>*, we can project the data points onto the *u<sup>1</sup>* unit vector, resulting in a 1-dimensional vector with values representing how far each projection is along the direction of *u<sup>1</sup>*. That way, we can downscale our dataset from 2 dimensions to only 1 dimension while still highly preserving the variance in data, as opposed to simply eliminating one of the features, which would cause a considerable decrease in variance and an unnacurate representation of the data. See comparison below.\n",
    "<div style=\"width: 100%; text-align: center; height: 70%; overflow: hidden\">\n",
    "<img src=\"https://i2.lensdump.com/i/tqwHfr.png\" style=\"width: 85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db5106",
   "metadata": {
    "papermill": {
     "duration": 0.028079,
     "end_time": "2022-06-01T18:52:42.141311",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.113232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>Here we can see how much clearer we are able to spot the clusters on the left plot even after reduction, that is because PCA is much better at preserving variation within the data in comparison to simply excluding features. That way we don't lose as many of the characteristics in our data and are safe to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47899094",
   "metadata": {
    "papermill": {
     "duration": 0.028609,
     "end_time": "2022-06-01T18:52:42.198246",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.169637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The covariance matrix\n",
    "<font size=3>\n",
    "    \n",
    "Through a more technical perspective, PCA can be thought as the Singular Value Decomposition (SVD) of the covariance matrix for a dataset. Therefore it would be useful to start by defining the covariance matrix, which is simply a square matrix representing the **covariance between two variables** for every variable in a dataset. In other words, how each feature relates to each of the other features.\n",
    "\n",
    "In order to calculate the covariance matrix, though, we first need to **normalize** from the dataset so that we only keep information about the quantitative covariance between variables, ignoring the linear magnitudes in those relations; this is equivalent to centralizing the data. Think about it, why would we want to perform calculations between vectors when they are all concentrated in only one quadrant of the graph? That would suggest there are redundancies in the data, so we wanna get rid of those by **subtracting the mean**.\n",
    "    \n",
    "Let's try to see the covariance between two variables as the product of those variables, such that <i>cov(x,y) = x * y</i>. If we now have several examples for *x* and *y* and want to get the variance between *x* and *y* for all these examples, we will need to centralize the data and then simply get the average variance: <i>cov(x,y) = 1/n * Σ(x<sup>(i)</sup> - x̄) * (y<sup>(i)</sup> - ȳ)</i>. However, recall that most frequently samples have way more features than only *x* and *y*, therefore in order to calculate the covariance between every combination of variables (the covariance matrix) we can think of *x<sup>(i)</sup>* as a *Nx1* vector with *N* features and then get the dot product of *x<sup>(i)</sup>* as *x<sup>(i)</sup>x<sup>(i)T</sup>*, which will result in a *NxN* matrix, think about it for a second. That way, we can finally write out the formula for the covariance matrix (*S*) as follows:\n",
    "    \n",
    "**S = 1/n * Σ(x<sup>(i)</sup> - x̄)(x<sup>(i)</sup> - x̄)<sup>T</sup>**\n",
    "    \n",
    "We must keep that formula since it will be useful later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd543c5b",
   "metadata": {
    "papermill": {
     "duration": 0.028523,
     "end_time": "2022-06-01T18:52:42.255419",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.226896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Obtaining components\n",
    "<font size=3>\n",
    "    \n",
    "Since we want to find a way in which to display the data as keeping the highest possible variance, we need a way to maximize the variance of the projected points with respect to the component vector we are trying to use. In other words, what vector maximizes the variance of points projected on it?\n",
    "\n",
    "Given the variance function <b><i>σ² = 1/n * Σ(x<sup>(i)</sup> − x̅)²</i></b> (the average of all squared differences between each term and the global mean, being squared so that we only get positive values), we can think of finding its maxima with respect to some variable as a task for Lagrange multipliers.\n",
    "    \n",
    "The method of Lagrange multipliers requires us to choose a function *f(x)* to be maximized (or minimized) and a constraint equation *g(x) = 0* to which the first function is subject, then we apply both to the formula ***L(x,λ) = f(x) + λg(x)***. The next step will be to take the first partial derivative of that formula and set it to zero, as we would do in a regular regression problem (which simply means trying to set the slope to zero, meaning we have found a peak).\n",
    "    \n",
    "In this case, applying our variables, **f(x)** will be <b><i>σ² = 1/n * Σ(u<sup>T</sup>x<sup>(i)</sup> − u<sup>T</sup>x̅)²</i></b> &ndash; *n* is the amount of samples; Σ operates from *i=0* up to *i=n*; *u<sup>T</sup>x* is a point projection; *u<sup>T</sup>x̅* is the mean projection &ndash;, thus we have the variance of the projected points, and the **constraint equation** will be ***u<sup>T</sup>u = 1***, since we are going to derive with respect to *u* and because every component *u* is a direction unit vector its magnitude needs to equal one (*||u|| = 1 = 1² = √(u<sub>1</sub>² + u<sub>2</sub>²)² = u<sub>1</sub>² + u<sub>2</sub>² = <b>u<sup>T</sup>u = 1</b>*). Finally, we can formulate the equation as follows:\n",
    "  \n",
    "L(x,λ) = f(x) + λg(x)<br>\n",
    "**L(x,λ) = 1/n * Σ(u<sup>T</sup>x<sup>(i)</sup> − u<sup>T</sup>x̅)² + λ(1 - u<sup>T</sup>u)**\n",
    "\n",
    "Now we only have to take the first partial derivative w.r.t *u* and set it to zero:\n",
    "\n",
    "L(x,λ)' = 0<br>\n",
    "dxdu 1/n * Σ(u<sup>T</sup>x<sup>(i)</sup> − u<sup>T</sup>x̅)² + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "dxdu 1/n * Σ**[u<sup>T</sup>(x<sup>(i)</sup> − x̅)]²** + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "dxdu 1/n * Σ**[u<sup>T</sup>(x<sup>(i)</sup> − x̅)(x<sup>(i)</sup> − x̅)<sup>T</sup>u]** + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "dxdu **u<sup>T</sup>u** * 1/n * Σ[(x<sup>(i)</sup> − x̅)(x<sup>(i)</sup> − x̅)<sup>T</sup>] + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "    \n",
    "If we look carefully, we can spot something that looks familiar in that equation and that is the formula of the covariance matrix *S = 1/n * Σ(x<sup>(i)</sup> - x̄)(x<sup>(i)</sup> - x̄)<sup>T</sup>*. Let's substitute it:\n",
    "    \n",
    "dxdu u<sup>T</sup>u * **1/n * Σ[(x<sup>(i)</sup> − x̅)(x<sup>(i)</sup> − x̅)<sup>T</sup>]** + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "dxdu u<sup>T</sup>**S**u + λ(1 - u<sup>T</sup>u) = 0<br>\n",
    "dxdu u<sup>T</sup>Su + **λ - u<sup>T</sup>λu** = 0<br>\n",
    "dxdu u<sup>T</sup>Su - **u<sup>T</sup>λu** = 0<br>\n",
    "**2Su** - **2λu** = 0<br>\n",
    "2Su = **2λu**<br>\n",
    "<b>Su = λu</b><br>\n",
    "    \n",
    "At the end, we see that by setting first partial derivative of our Lagrange formula w.r.t *u* to zero we get **Su = λu**. If that equation looks familiar to you, you may already be starting to visualize where we are going. In fact, that happens to be precisely the equation of eigenvalues, in which ***λ* (lambda) and *u* are the eigenvalue and eigenvector of *S*, respectively**. And that is why PCA is sometimes referred to as the Singular Value Decomposition of the covariance matrix, since we now only have to resolve the eigendecomposition of the square matrix S.\n",
    "    \n",
    "We won't be resolving the equation here since it can quickly evolve into a high polynomial problem which would take too long to solve, however keep in mind that once it's done we are going to end up with a set of eigenvalues, each associated with an eigenvector, and those eigenvector make out or components. That lead us to the final question in this topic: how to choose which eigenvector to use as the principal component? Let us go back to the last piece of equation we have written and multiply it by *u* so we can work on it a bit. Remind that ***u<sup>T</sup>u = 1***.\n",
    "    \n",
    "Su = λu<br>\n",
    "**(u)** Su = λu<br>\n",
    "**u<sup>T</sup>**Su = **u<sup>T</sup>**λu<br>\n",
    "u<sup>T</sup>Su = **λ**<br>\n",
    "**u<sup>T</sup>u * 1/n * Σ[(x<sup>(i)</sup> − x̅)(x<sup>(i)</sup> − x̅)<sup>T</sup>] = λ**\n",
    "    \n",
    "At the final we had the chance to go back into our original formula for the variance, the same formula we were trying to maximize, and now it is equal to λ. What does that mean? If we want **to maximize the left side of the equation, that is the variance function, we only have to maximize the right side, *λ*; therefore, for the principal component we are going to use the eigenvector with the largest eigenvalue assigned to it**. In case we need more than one component, though, similarly we should choose them by their eigenvalues. These are called principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088246cf",
   "metadata": {
    "papermill": {
     "duration": 0.028212,
     "end_time": "2022-06-01T18:52:42.312134",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.283922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilizing the principal components\n",
    "<font size=3>\n",
    "    \n",
    "Each eigenvector corresponds to certain dimension in the final *z<sup>(i)</sup>* transformed sample and is also capable of preserving only a specific amount of variance if we were keep it, with respect to its eigenvalue. Essentially what a component does is to best express the data up to the dimension it is assigned to, how well it does that is defined by how large its eigenvalue is. The projections of one sample onto every chosen components will result in a new sample with its features as the results of those linear transformations. In case we need more than one component, let's say we were trying to reduce one sample from an *Nx1* vector to a *Kx1*, such that *K &lt; N*, then we would simply have to sort our set of eigenvectors by their eigenvalues, from largest to lowest, and pick the first *K* components which represent the number of variables we want to keep. Finally, we would perform a series of linear transformations (dot product) on the sample's features by our chosen principal components, in which each transformation outputs a single variable that best explains the data up to that dimension. In other words, given a set of principal components *u1,...,uK* (NxK), we have that <b><i>z<sup>(i)</sup> = u<sup>T</sup> * x<sup>(i)</sup></i></b>, with *z<sup>(i)</sup>* as shape *Kx1*.\n",
    "\n",
    "In order to calculate the total variance retained, we have to take the sum of all eigenvalues assigned to our principal components *u<sup>(1)</sup>,...,u<sup>(K)</sup>* and divide it by the number of terms; therefore, the total variance retained in the dataset can be expressed as <b><i>σ² = 1/K * Σ(λ<sup>(i)</sup>)</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d557dcf",
   "metadata": {
    "papermill": {
     "duration": 0.028096,
     "end_time": "2022-06-01T18:52:42.368650",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.340554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8eb77b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.055876,
     "end_time": "2022-06-01T18:52:42.453139",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.397263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd1193",
   "metadata": {
    "papermill": {
     "duration": 0.044381,
     "end_time": "2022-06-01T18:52:42.542959",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.498578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c301f5",
   "metadata": {
    "papermill": {
     "duration": 1.577193,
     "end_time": "2022-06-01T18:52:44.165352",
     "exception": false,
     "start_time": "2022-06-01T18:52:42.588159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import glob\n",
    "import random\n",
    "import tarfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840f255",
   "metadata": {
    "papermill": {
     "duration": 0.043118,
     "end_time": "2022-06-01T18:52:44.252126",
     "exception": false,
     "start_time": "2022-06-01T18:52:44.209008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting up data\n",
    "\n",
    "<font size=3> The dataset we will be using is the **LFW - People (Face Recognition)**\n",
    "    \n",
    "source: https://www.kaggle.com/datasets/atulanandjha/lfwpeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ee3f02",
   "metadata": {
    "papermill": {
     "duration": 9.350145,
     "end_time": "2022-06-01T18:52:53.645712",
     "exception": false,
     "start_time": "2022-06-01T18:52:44.295567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir('kaggle/input/lfwpeople')\n",
    "\n",
    "with tarfile.open('kaggle/input/lfwpeople/lfw-funneled.tgz') as tar:\n",
    "    tar.extractall() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528bfb6",
   "metadata": {
    "papermill": {
     "duration": 0.028988,
     "end_time": "2022-06-01T18:52:53.705396",
     "exception": false,
     "start_time": "2022-06-01T18:52:53.676408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>\n",
    "We can use a custom function which is going to get a list of paths to each image in the dataset and output a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b896638",
   "metadata": {
    "papermill": {
     "duration": 0.046187,
     "end_time": "2022-06-01T18:52:53.780841",
     "exception": false,
     "start_time": "2022-06-01T18:52:53.734654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setupData(paths, img_size, generate_labels=True):\n",
    "    '''\n",
    "    Input\n",
    "    -------------\n",
    "    paths : list of strings\n",
    "        List of string paths regarding image files.\n",
    "    img_shape : tuple of ints (n, m, 3)\n",
    "        Tuple that represents the output shape for each image in the numpy matrix. The last\n",
    "        dimension has to equal 3 as for the RGB channel.\n",
    "    generate_labels : bool, optional, default: True\n",
    "        Whether or not to return a 'labels' dict which translates 'y' int data into string labels.\n",
    "        \n",
    "    Output\n",
    "    -------------\n",
    "    X : numpy array, shape: (n, m, 3)\n",
    "        X input data, a matrix with (n, m) dimensions and (3) 3-dimensional color channel.\n",
    "    y : numpy array\n",
    "        y input data, a single dimension array.\n",
    "    '''\n",
    "    n = len(paths)\n",
    "    X_data = np.zeros(np.insert(img_size, 0, n), dtype='float32')\n",
    "    y_data = np.zeros(n, dtype='int16')\n",
    "\n",
    "    common_path = os.path.commonpath(paths)\n",
    "    labels = dict([(i, label) for i, label in enumerate(os.listdir(common_path))])\n",
    "\n",
    "    print(f'Data shape -> X: {X_data.shape}, y: {y_data.shape} \\n---------------------\\n')\n",
    "    \n",
    "    inverse_labels = {item[1]:item[0] for item in labels.items()} # Get inverse dict for string search\n",
    "     \n",
    "    for i in range(n):\n",
    "        img = cv2.imread(paths[i], cv2.IMREAD_COLOR)[:, :, ::-1] # Read image file and invert BGR to RGB\n",
    "        X_data[i, :, :, :] = cv2.resize(img, img_size[:2], interpolation=cv2.INTER_AREA) / 255\n",
    "\n",
    "        label = os.path.relpath(paths[i], common_path).partition('/')[0] # Get label from relative path\n",
    "        y_data[i] = inverse_labels[label]\n",
    "        \n",
    "    return (X_data, y_data, labels) if generate_labels else (X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7deee",
   "metadata": {
    "papermill": {
     "duration": 0.045118,
     "end_time": "2022-06-01T18:52:53.870153",
     "exception": false,
     "start_time": "2022-06-01T18:52:53.825035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>\n",
    "Now that we have our function, we can call it to set up the data into X_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f404a59",
   "metadata": {
    "papermill": {
     "duration": 3.954109,
     "end_time": "2022-06-01T18:52:57.868780",
     "exception": false,
     "start_time": "2022-06-01T18:52:53.914671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "commonpath() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m paths \u001b[38;5;241m=\u001b[39m [path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/lfw_funneled/**/*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)][:\u001b[38;5;241m2000\u001b[39m]\n\u001b[0;32m      3\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m125\u001b[39m, \u001b[38;5;241m125\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m X_data, y_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43msetupData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(X_data[\u001b[38;5;241m0\u001b[39m]), labels[y_data[\u001b[38;5;241m0\u001b[39m]]\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36msetupData\u001b[1;34m(paths, img_size, generate_labels)\u001b[0m\n\u001b[0;32m     21\u001b[0m X_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(np\u001b[38;5;241m.\u001b[39minsert(img_size, \u001b[38;5;241m0\u001b[39m, n), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m y_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m common_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommonpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(i, label) \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(common_path))])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData shape -> X: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py:753\u001b[0m, in \u001b[0;36mcommonpath\u001b[1;34m(paths)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m\"\"\"Given a sequence of path names, returns the longest common sub-path.\"\"\"\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[1;32m--> 753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommonpath() arg is an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    755\u001b[0m paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(os\u001b[38;5;241m.\u001b[39mfspath, paths))\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(paths[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: commonpath() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# paths = [path for path in glob.glob('/kaggle/working/lfw_funneled/**/*.jpg')][:2000]\n",
    "paths = [path for path in glob.glob('/kaggle/working/lfw_funneled/**/*.jpg')][:2000]\n",
    "\n",
    "shape = (125, 125, 3)\n",
    "\n",
    "X_data, y_data, labels = setupData(paths, img_size=shape, generate_labels=True)\n",
    "\n",
    "plt.imshow(X_data[0]), labels[y_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab546b",
   "metadata": {
    "papermill": {
     "duration": 0.238557,
     "end_time": "2022-06-01T18:52:58.137959",
     "exception": false,
     "start_time": "2022-06-01T18:52:57.899402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a random sample to check dataset.\n",
    "random_sample = random.randint(0, len(paths))\n",
    "plt.imshow(X_data[random_sample]), labels[y_data[random_sample]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6d5b5",
   "metadata": {
    "papermill": {
     "duration": 0.03252,
     "end_time": "2022-06-01T18:52:58.203582",
     "exception": false,
     "start_time": "2022-06-01T18:52:58.171062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>Here we are going to define all the functions and variables that make up the PCA algorithm inside a class. We can use NumPy's Linear Algebra library (numpy.linalg) for the eigendecomposition, by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95e363",
   "metadata": {
    "papermill": {
     "duration": 0.053371,
     "end_time": "2022-06-01T18:52:58.289903",
     "exception": false,
     "start_time": "2022-06-01T18:52:58.236532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PCA_Custom:\n",
    "    '''\n",
    "        ----------------\n",
    "        PCA algorithm.\n",
    "        ----------------\n",
    "    '''\n",
    "    def __init__(self, k=0):\n",
    "        '''\n",
    "            Input\n",
    "            -------------------\n",
    "                k : int, optional, default: 0\n",
    "                    Number of principal components to be generated during the fit process. The amount \n",
    "                    of components also corresponds to the number of features in the output data.\n",
    "                    If 'k' is in between 0.0 and 1.0 (exclusive), then selects highest number of components\n",
    "                    which preserves at least 'k' amount of variance. If no value is passed, will use the \n",
    "                    maximum number of components, corresponding to all features.\n",
    "            Output\n",
    "            -------------------\n",
    "                PCA model object.\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.mean = 0\n",
    "    \n",
    "    def _calcCovarianceMatrix(self, X):\n",
    "        self.mean = X.mean(axis=1).reshape(-1, 1)\n",
    "        X_norm = X - self.mean\n",
    "        S = X_norm @ X_norm.T / X_norm.shape[1]\n",
    "        return S\n",
    "    \n",
    "    def _calcEigs(self, S, k):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(S)\n",
    "        u = eigenvectors[:, np.argsort(eigenvalues)][:, ::-1][:, :k]\n",
    "        self.variance = sum(eigenvalues[:k]) / sum(eigenvalues)\n",
    "        return u\n",
    "    \n",
    "    def fit(self, X):\n",
    "        ''' \n",
    "           Input\n",
    "            -----------------\n",
    "                X : numpy array\n",
    "                    X input data numpy array (n_featues, n_samples).\n",
    "        '''\n",
    "        S = self._calcCovarianceMatrix(X)\n",
    "        n = X.shape[0]\n",
    "        self.original_shape = X.shape\n",
    "        \n",
    "        if 0.0 < self.k < 1.0: # Test variance for different amounts of components until variance >= k\n",
    "            for num_k in range(1, n + 1):\n",
    "                temp_components = self._calcEigs(S, num_k)\n",
    "                if self.variance >= self.k:\n",
    "                    self.components = temp_components\n",
    "                    self.k = num_k\n",
    "                    break\n",
    "            if self.k < 1:\n",
    "                raise Exception(f\"Could not achieve variance {self.k}.\")\n",
    "            \n",
    "        elif self.k == 0:\n",
    "            self.k = n\n",
    "            self.components = self._calcEigs(S, self.k)\n",
    "        else:\n",
    "            self.components = self._calcEigs(S, self.k)\n",
    "            \n",
    "        \n",
    "    def transform(self, X):\n",
    "        ''' \n",
    "           Input\n",
    "            -----------------\n",
    "                X : numpy array\n",
    "                    X input data numpy array.\n",
    "        '''\n",
    "        z = self.components.T @ (X - self.mean)\n",
    "        return z\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        ''' \n",
    "           Input\n",
    "            -----------------\n",
    "                X : numpy array\n",
    "                    X input data numpy array.\n",
    "        '''\n",
    "        X_raw = self.components @ X + self.mean\n",
    "        return X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a760d6c",
   "metadata": {
    "papermill": {
     "duration": 0.049337,
     "end_time": "2022-06-01T18:52:58.389274",
     "exception": false,
     "start_time": "2022-06-01T18:52:58.339937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compare SciKit-learn PCA to our custom PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ab9e1",
   "metadata": {
    "papermill": {
     "duration": 0.057532,
     "end_time": "2022-06-01T18:52:58.496543",
     "exception": false,
     "start_time": "2022-06-01T18:52:58.439011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = X_data.shape[0]\n",
    "X_data.reshape(-1, 125).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fff183",
   "metadata": {
    "papermill": {
     "duration": 4.756061,
     "end_time": "2022-06-01T18:53:03.302328",
     "exception": false,
     "start_time": "2022-06-01T18:52:58.546267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scikit-learn PCA\n",
    "\n",
    "pca = PCA(0.99) # Try to retain 99% variance\n",
    "\n",
    "pca.fit(X_data.reshape(-1, 125))\n",
    "\n",
    "print(f\"Output shape: {pca.components_.shape} || Retained variance: {pca.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60e86c",
   "metadata": {
    "papermill": {
     "duration": 0.878345,
     "end_time": "2022-06-01T18:53:04.231309",
     "exception": false,
     "start_time": "2022-06-01T18:53:03.352964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Custom PCA\n",
    "\n",
    "pca_custom = PCA_Custom(k=0.99) # Try to retain 99% variance\n",
    "\n",
    "pca_custom.fit(X_data.reshape(-1, 125).T)\n",
    "\n",
    "print(f\"Output shape: {pca_custom.components.shape} || Retained variance: {pca_custom.variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8dbcb",
   "metadata": {
    "papermill": {
     "duration": 0.059312,
     "end_time": "2022-06-01T18:53:04.352399",
     "exception": false,
     "start_time": "2022-06-01T18:53:04.293087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>The total variance retained during transformations is of about 99% in both models. That means our data will be 1% less accurate afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d13f85",
   "metadata": {
    "papermill": {
     "duration": 0.061429,
     "end_time": "2022-06-01T18:53:04.464711",
     "exception": false,
     "start_time": "2022-06-01T18:53:04.403282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca.components_[0][:5], pca_custom.components.T[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62667e47",
   "metadata": {
    "papermill": {
     "duration": 0.05057,
     "end_time": "2022-06-01T18:53:04.566230",
     "exception": false,
     "start_time": "2022-06-01T18:53:04.515660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>Both output very similar components as well, which means our algorithm seems to be working right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d284531",
   "metadata": {
    "papermill": {
     "duration": 0.052506,
     "end_time": "2022-06-01T18:53:04.669738",
     "exception": false,
     "start_time": "2022-06-01T18:53:04.617232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compression transform and decompression inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b592718",
   "metadata": {
    "papermill": {
     "duration": 10.13272,
     "end_time": "2022-06-01T18:53:14.855299",
     "exception": false,
     "start_time": "2022-06-01T18:53:04.722579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xz = pca_custom.transform(X_data.reshape(-1, 125).T).astype('float32')\n",
    "X_restored = pca_custom.inverse_transform(Xz).T.reshape(X_data.shape).astype('float32')\n",
    "X_restored.shape, pca_custom.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63023118",
   "metadata": {
    "papermill": {
     "duration": 0.035059,
     "end_time": "2022-06-01T18:53:14.925124",
     "exception": false,
     "start_time": "2022-06-01T18:53:14.890065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>Comparing data size between original and compressed data. Also decompressed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5284f",
   "metadata": {
    "papermill": {
     "duration": 0.047348,
     "end_time": "2022-06-01T18:53:15.007743",
     "exception": false,
     "start_time": "2022-06-01T18:53:14.960395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "byte_sizes = {'X_reduced': Xz.nbytes / 10e6, \n",
    "          'X_original': X_data.nbytes / 10e6, \n",
    "          'X_restored': X_restored.nbytes / 10e6}\n",
    "\n",
    "for i in byte_sizes.items():\n",
    "    print(f'{i[0]}: {i[1]}mb')\n",
    "\n",
    "print('-' * 20)\n",
    "print(f\"Data reduced by: {100 - byte_sizes['X_reduced'] / byte_sizes['X_original'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652c808",
   "metadata": {
    "papermill": {
     "duration": 0.042899,
     "end_time": "2022-06-01T18:53:15.093560",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.050661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>We can check that we have achieved around 76.8% of reduction in memory usage in comparison to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8acc0",
   "metadata": {
    "papermill": {
     "duration": 0.035164,
     "end_time": "2022-06-01T18:53:15.165238",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.130074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>Let's now check the integrity of our image sample after decompression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cc451",
   "metadata": {
    "papermill": {
     "duration": 0.571077,
     "end_time": "2022-06-01T18:53:15.771431",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.200354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, dpi=100)\n",
    "\n",
    "ax[0].imshow(X_restored[0].reshape(125, 125, 3))\n",
    "ax[0].set_title(\"Restored X sample\")\n",
    "ax[1].imshow(X_data[0].reshape(125, 125, 3))\n",
    "ax[1].set_title(\"Original X sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d9373",
   "metadata": {
    "papermill": {
     "duration": 0.048936,
     "end_time": "2022-06-01T18:53:15.859391",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.810455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data[0][50][:5], X_restored[0][50][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4940a1",
   "metadata": {
    "papermill": {
     "duration": 0.038573,
     "end_time": "2022-06-01T18:53:15.937438",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.898865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5ced9",
   "metadata": {
    "papermill": {
     "duration": 0.038504,
     "end_time": "2022-06-01T18:53:16.015213",
     "exception": false,
     "start_time": "2022-06-01T18:53:15.976709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating plots for markdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628c890",
   "metadata": {
    "papermill": {
     "duration": 0.038803,
     "end_time": "2022-06-01T18:53:16.093316",
     "exception": false,
     "start_time": "2022-06-01T18:53:16.054513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>A function to procedurally generate plots used in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0a9cd",
   "metadata": {
    "papermill": {
     "duration": 2.212552,
     "end_time": "2022-06-01T18:53:18.344929",
     "exception": false,
     "start_time": "2022-06-01T18:53:16.132377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_custom_mk = PCA_Custom(1)\n",
    "\n",
    "initial = np.array([[0.1, 0.1]])\n",
    "distribution = pd.DataFrame(initial)\n",
    "\n",
    "for i in range(1, 31):\n",
    "    noise = 0.5 * np.random.rand(1, 2)**2\n",
    "    sample = noise + i / 30\n",
    "    distribution = distribution.append(pd.DataFrame(sample))\n",
    "    \n",
    "distribution -= distribution.mean(axis=0)\n",
    "\n",
    "pca_custom_mk.fit(distribution.to_numpy().T)\n",
    "comps_mk = pca_custom_mk.components.flatten()\n",
    "lowered_distribution = pca_custom_mk.transform(distribution.to_numpy().T)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), dpi=100)\n",
    "\n",
    "ax[0,0].axline((0,0), comps_mk)\n",
    "for i in range(len(distribution)):\n",
    "    point1, point2 = distribution.iloc[i], lowered_distribution.T[i] * comps_mk\n",
    "    x1 = [point1[0], point2[0]]\n",
    "    y1 = [point1[1], point2[1]]\n",
    "    ax[0,0].plot(x1, y1, linestyle=':')\n",
    "    ax[0,0].scatter(x1[0], y1[0])\n",
    "    ax[1,0].scatter(lowered_distribution.T[i], [0], marker='o', zorder=1)\n",
    "    ax[1,0].axhline(0, -3, 3, zorder=0)\n",
    "    \n",
    "    x2 = [point1[0], point1[0]]\n",
    "    y2 = [point1[1], 0]\n",
    "    ax[0,1].plot(x2, y2, linestyle=':')\n",
    "    ax[0,1].scatter(x2[0], y2[0])\n",
    "    ax[0,1].axhline(0, -3, 3, zorder=0)\n",
    "    ax[1,1].scatter(point1[0], [0], marker='o', zorder=1)\n",
    "    ax[1,1].axhline(0, -3, 3, zorder=0)\n",
    "\n",
    "ax[0,0].set_xlabel('x1')\n",
    "ax[0,0].set_ylabel('x2')       \n",
    "ax[0,0].set_title('Original data projected onto component')\n",
    "ax[0,1].set_title('Original data discarding x2 feature')\n",
    "ax[0,1].set_ylabel('x2')\n",
    "ax[0,1].set_xlabel('x1')\n",
    "ax[1,0].set_title('z¹ vector')\n",
    "ax[1,1].set_title('x¹ feature vector')\n",
    "\n",
    "for i in [(0,0), (0,1), (1,0), (1,1)]:\n",
    "    ax[i].axis('equal')\n",
    "    \n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    ax[i].set_ylim(-1.0, 1.0)\n",
    "    ax[i].set_xlim(-3, 3)\n",
    "    \n",
    "plt.savefig('pca_plot1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974012e0",
   "metadata": {
    "papermill": {
     "duration": 0.040639,
     "end_time": "2022-06-01T18:53:18.426968",
     "exception": false,
     "start_time": "2022-06-01T18:53:18.386329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Random K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce756c0",
   "metadata": {
    "papermill": {
     "duration": 0.041898,
     "end_time": "2022-06-01T18:53:18.509786",
     "exception": false,
     "start_time": "2022-06-01T18:53:18.467888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<font size=3>A random K-means algorithm which will maybe be explored in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550f365",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.061026,
     "end_time": "2022-06-01T18:53:18.612366",
     "exception": false,
     "start_time": "2022-06-01T18:53:18.551340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    '''\n",
    "    ----------------------\n",
    "        K-Means algorithm.\n",
    "    ----------------------\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, k=10):\n",
    "        '''\n",
    "        Input\n",
    "        ----------------\n",
    "            k : int, optional, default: 10\n",
    "                Number of clusters to be used in K-means.\n",
    "        Output\n",
    "        ----------------\n",
    "            K-means model object.\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.K = np.zeros(k, dtype='float32')\n",
    "        \n",
    "    \n",
    "    def _randomlyInizializeClusters(self, X):\n",
    "        m = len(X)\n",
    "        self.K = [X[random.randint(0, m)] for i in range(self.k)]\n",
    "    \n",
    "    def _clusterAssignment(self, X):\n",
    "        m = len(X)\n",
    "        for i in range(m):\n",
    "            costs = [((X[i] - uk)**2).sum() for uk in self.K] # Compute distance cost for all clusters\n",
    "            self.c[i] = costs.index(min(costs)) # Get cluster index of closest cluster\n",
    "            \n",
    "    def _moveCentroid(self, X):\n",
    "        for uk in range(self.k):\n",
    "            self.K[uk] = X[self.c == uk].mean(axis=0) # Get average position of points assigned to cluster\n",
    "    \n",
    "    def fit(self, X, epochs=100, verbose=True):\n",
    "        '''\n",
    "            Input\n",
    "            -----------------\n",
    "                X : numpy array\n",
    "                    X input data numpy array.\n",
    "                epochs : int, optional, default: 100\n",
    "                    Number of epoch iterations used during training.\n",
    "                verbose: bool, optional, default: True\n",
    "                    Whether or not to output information and display the progress bar.\n",
    "        '''\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        assert isinstance(X, np.ndarray)\n",
    "        \n",
    "        self.c = np.zeros(len(X), dtype='int16') # Array of cluster index assigned to each sample\n",
    "        self._randomlyInizializeClusters(X)\n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(epochs), desc='epochs', disable=not verbose):\n",
    "            self._clusterAssignment(X)\n",
    "            self._moveCentroid(X)\n",
    "        print(f'\\n{epochs} iterations finished.')\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert isinstance(X, np.ndarray)\n",
    "        \n",
    "        m = len(X)\n",
    "        self._clusterAssignment(X)\n",
    "        plt.imshow(self.K[self.c[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c2a0d",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.047548,
     "end_time": "2022-06-01T18:53:18.701691",
     "exception": false,
     "start_time": "2022-06-01T18:53:18.654143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Kmeans(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf470973",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 78.75804,
     "end_time": "2022-06-01T18:54:37.500098",
     "exception": false,
     "start_time": "2022-06-01T18:53:18.742058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X_data[:2000], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5009f1",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.343991,
     "end_time": "2022-06-01T18:54:37.951366",
     "exception": false,
     "start_time": "2022-06-01T18:54:37.607375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(X_data[67]), model.c[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.676333,
   "end_time": "2022-06-01T18:54:38.984195",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-01T18:52:33.307862",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
